{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装依赖（torch版本需要自己调整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "!pip install trl modelscope transformers -i https://mirrors.aliyun.com/pypi/simple/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen2.5-0.5B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref和policy模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"C:\\\\Users\\\\owen\\\\.cache\\\\modelscope\\\\hub\\\\Qwen\\\\Qwen2___5-0___5B-Instruct\"\n",
    "\n",
    "policy = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ref = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([151644,   8948,    271,    262,  39533,    304,    279,   2701,   3561,\n",
       "            510,    262,    366,  19895,    287,    397,    262,  12236,    262,\n",
       "            690,  19895,    287,    397,    262,    366,   9217,    397,    262,\n",
       "          12236,    262,    690,   9217,    397,    257, 151645,    198, 151644,\n",
       "            872,    198,     18,     10,     20, 107106,  99195, 101036,  11319,\n",
       "         151645,    198, 151644,  77091,    198], device='cuda:0'),\n",
       " '<|im_start|>system\\n\\n    Respond in the following format:\\n    <reasoning>\\n    ...\\n    </reasoning>\\n    <answer>\\n    ...\\n    </answer>\\n    <|im_end|>\\n<|im_start|>user\\n3+5等于几呢？<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " tensor([    18,    488,    220,     20,    284,    220,     23, 151645],\n",
       "        device='cuda:0'),\n",
       " '3 + 5 = 8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def gen_completion(model,query,tokenizer):\n",
    "    SYSTEM='''\n",
    "    Respond in the following format:\n",
    "    <reasoning>\n",
    "    ...\n",
    "    </reasoning>\n",
    "    <answer>\n",
    "    ...\n",
    "    </answer>\n",
    "    '''\n",
    "    messages=[{'role':'system','content':SYSTEM},{'role':'user','content':query}]\n",
    "    text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs\n",
    "        )\n",
    "    completion_ids=generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "    completion_text=tokenizer.decode(completion_ids, skip_special_tokens=True)\n",
    "    return model_inputs.input_ids[0],text,completion_ids,completion_text\n",
    "\n",
    "gen_completion(policy,'3+5等于几呢？',tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算model预测的目标token概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11]) torch.Size([11]) 3 + 5 等于8。\n"
     ]
    }
   ],
   "source": [
    "input_ids,input_text,completion_ids,completion_text=gen_completion(policy,'3+5等于几呢？',tokenizer)\n",
    "\n",
    "def gen_probs(model,input_ids,completion_ids):\n",
    "    full_ids=torch.cat([input_ids,completion_ids],dim=0).unsqueeze(0)\n",
    "    output=model(full_ids)\n",
    "    probs=torch.nn.functional.softmax(output.logits,dim=-1)\n",
    "    prob_select=full_ids[:,1:].unsqueeze(-1)\n",
    "    token_probs=torch.gather(probs[:,:-1],dim=-1,index=prob_select)[0]\n",
    "    return token_probs[len(input_ids)-1:,0]\n",
    "\n",
    "ref_token_probs=gen_probs(ref,input_ids,completion_ids)\n",
    "print(ref_token_probs.shape,completion_ids.shape,completion_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
