{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装依赖（torch版本需要自己调整）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - pytorch\n",
      " - nvidia\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owen\\miniconda3\\Lib\\site-packages\\conda\\base\\context.py:198: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
      "\n",
      "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
      "\n",
      "  conda config --add channels defaults\n",
      "\n",
      "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
      "\n",
      "  deprecated.topic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: trl in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: modelscope in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (1.22.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (4.48.3)\n",
      "Requirement already satisfied: addict in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from trl) (1.3.0)\n",
      "Requirement already satisfied: datasets>=2.21.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from trl) (3.2.0)\n",
      "Requirement already satisfied: rich in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: requests>=2.25 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from modelscope) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from datasets>=2.21.0->trl) (3.11.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from requests>=2.25->modelscope) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from requests>=2.25->modelscope) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from tqdm>=4.64.0->modelscope) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "!pip install trl modelscope transformers addict -i https://mirrors.aliyun.com/pypi/simple/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\owen\\.cache\\modelscope\\hub\\Qwen\\Qwen2.5-1.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 09:56:10,042 - modelscope - INFO - Creating symbolic link [C:\\Users\\owen\\.cache\\modelscope\\hub\\Qwen\\Qwen2.5-1.5B-Instruct].\n",
      "2025-02-09 09:56:10,042 - modelscope - WARNING - Failed to create symbolic link C:\\Users\\owen\\.cache\\modelscope\\hub\\Qwen\\Qwen2.5-1.5B-Instruct for C:\\Users\\owen\\.cache\\modelscope\\hub\\Qwen\\Qwen2.5-1.5B-Instruct\\Qwen\\Qwen2___5-1___5B-Instruct.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('Qwen/Qwen2.5-1.5B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref和policy模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 根据自己电脑~/.cache位置修改\n",
    "model_name = \"C:\\\\Users\\\\owen\\\\.cache\\\\modelscope\\\\hub\\\\Qwen\\\\Qwen2___5-1___5B-Instruct\"\n",
    "\n",
    "policy = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ref = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owen\\miniconda3\\envs\\llm_rl\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:53: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([151644,   8948,    271,    286,  23240,   5889,    304,    279,   2701,\n",
       "           3561,    510,    286,    366,  19895,    287,    397,    286,  12236,\n",
       "            286,    690,  19895,    287,    397,    286,    366,   9217,    397,\n",
       "            286,  12236,    286,    690,   9217,    397,    257, 151645,    198,\n",
       "         151644,    872,    198,  30709,  30858, 101935,  33108, 105367,  71268,\n",
       "         100373,  52510,  69249,   3837,  30709,  30858,  99730,  60726,  99509,\n",
       "         100165,     30, 151645,    198, 151644,  77091,    198],\n",
       "        device='cuda:0'),\n",
       " '<|im_start|>system\\n\\n        Always respond in the following format:\\n        <reasoning>\\n        ...\\n        </reasoning>\\n        <answer>\\n        ...\\n        </answer>\\n    <|im_end|>\\n<|im_start|>user\\n小明妈妈和老婆都掉水里，小明应该先救谁?<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " tensor([104596, 104705,   3837,  30709,  30858,  85106, 104747, 101118, 100005,\n",
       "          99464,   1773,  62244,  42411, 100006,  82894,  49828, 101447,  99234,\n",
       "           3837,  99730, 101140, 104482,  82894,  26939,  46944,  73670,  99464,\n",
       "          99164,  97145, 103958,   3837, 104610, 106455, 116133,   1773,  99519,\n",
       "          62244,  30709,  30858,  99250, 100199,  17254, 121510, 115738, 100631,\n",
       "         101068, 100359, 100005, 100675,   3837, 104309,  32664,  92894, 103947,\n",
       "         100702, 104384, 106155, 105204,   3407,  99999,   3837, 105045, 102349,\n",
       "          20412,   5122,  30709,  30858,  99730,  60726,  99509,  99283,   1773,\n",
       "         151645], device='cuda:0'),\n",
       " '在这个情况下，小明需要优先考虑自己的安全。如果他能够游得足够快，应该首先尝试游到一个可以安全着陆的地方，而不是试图救人。因为如果小明被卷入漩涡或者无法控制自己的行动，可能会对其他人的生命构成更大的威胁。\\n\\n所以，正确的答案是：小明应该先救自己。')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "def gen_completion(model,query,tokenizer):\n",
    "    SYSTEM='''\n",
    "        Always respond in the following format:\n",
    "        <reasoning>\n",
    "        ...\n",
    "        </reasoning>\n",
    "        <answer>\n",
    "        ...\n",
    "        </answer>\n",
    "    '''\n",
    "    messages=[{'role':'system','content':SYSTEM},{'role':'user','content':query}]\n",
    "    text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_length=5000,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    completion_ids=generated_ids[0][len(model_inputs.input_ids[0]):]\n",
    "    completion_text=tokenizer.decode(completion_ids, skip_special_tokens=True)\n",
    "    return model_inputs.input_ids[0],text,completion_ids,completion_text\n",
    "\n",
    "gen_completion(policy,'小明妈妈和老婆都掉水里，小明应该先救谁?',tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算model预测的目标token概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([47]) torch.Size([47]) <reasoning>\n",
      "加法是数学中的基本运算之一，用于计算两个数的总和。在这个问题中，我们需要将数字3和5相加。\n",
      "</reasoning>\n",
      "\n",
      "<answer>\n",
      "8\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "def gen_probs(model,input_ids,completion_ids):\n",
    "    full_ids=torch.cat([input_ids,completion_ids],dim=0).unsqueeze(0)\n",
    "    output=model(full_ids)\n",
    "    probs=torch.nn.functional.softmax(output.logits,dim=-1)\n",
    "    prob_select=full_ids[:,1:].unsqueeze(-1)\n",
    "    token_probs=torch.gather(probs[:,:-1],dim=-1,index=prob_select)[0]\n",
    "    return token_probs[len(input_ids)-1:,0]\n",
    "\n",
    "input_ids,input_text,completion_ids,completion_text=gen_completion(policy,'3+5等于几?',tokenizer)\n",
    "ref_token_probs=gen_probs(ref,input_ids,completion_ids)\n",
    "print(ref_token_probs.shape,completion_ids.shape,completion_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载gsm8k数学题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 09:56:21,420 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from gsm8k. Please make sure that you can trust the external codes.\n",
      "2025-02-09 09:56:21,687 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from modelscope/gsm8k. Please make sure that you can trust the external codes.\n",
      "2025-02-09 09:56:21,689 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from modelscope/gsm8k. Please make sure that you can trust the external codes.\n",
      "2025-02-09 09:56:21,690 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from modelscope/gsm8k. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72',\n",
       " 'final_answer': '72'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "\n",
    "# 提取答案\n",
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text:\n",
    "        return ''\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "ds =  MsDataset.load('modelscope/gsm8k', subset_name='main', split='train')\n",
    "ds = ds.map(lambda x: {'final_answer':extract_hash_answer(x['answer'])})\n",
    "\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(input_text,completion_text,answer):\n",
    "    score=0\n",
    "    \n",
    "    # 答案完全正确\n",
    "    model_answer = completion_text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "    if model_answer==answer:\n",
    "        score+=2\n",
    "        \n",
    "    # 答案只写数字\n",
    "    if model_answer.isdigit():\n",
    "        score+=0.5\n",
    "    \n",
    "    # 格式遵循\n",
    "    for tag in ['<reasoning>','</reasoning>','<answer>','</answer>']:\n",
    "        if completion_text.count(tag)==1:\n",
    "            score+=0.125\n",
    "    return score\n",
    "\n",
    "#reward_func('','<reasoning></reasoning><answer>10</answer>','10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRPO训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "import torch \n",
    "\n",
    "G=4 # 每个Query回答G次\n",
    "dataloader=DataLoader(ds,batch_size=1,shuffle=True) \n",
    "optimizer=SGD(policy.parameters(),lr=1e-4)\n",
    "\n",
    "while True:\n",
    "    for batch in dataloader:\n",
    "        batch_queries=batch['question']\n",
    "        batch_answers=batch['answer']\n",
    "        batch_final_answers=batch['final_answer']\n",
    "                \n",
    "        # 采样\n",
    "        batch_ref_policy_probs=[]\n",
    "        batch_completions=[]\n",
    "        for q_i in range(len(batch_queries)):\n",
    "            group_completions=[]\n",
    "            group_ref_policy_probs=[]\n",
    "            for o_i in range(G): \n",
    "                # old policy completion\n",
    "                completion=gen_completion(policy,batch_queries[q_i],tokenizer)\n",
    "                group_completions.append(completion)\n",
    "                # ref policy probs\n",
    "                input_ids,completion_ids=completion[0],completion[2]\n",
    "                ref_policy_probs=gen_probs(ref,input_ids,completion_ids)\n",
    "                group_ref_policy_probs.append(ref_policy_probs.detach())\n",
    "            batch_completions.append(group_completions)\n",
    "            batch_ref_policy_probs.append(group_ref_policy_probs)\n",
    "        \n",
    "        # 计算优势\n",
    "        batch_group_advantages=[]\n",
    "        batch_old_policy_probs=[]\n",
    "        for q_i in range(len(batch_queries)): \n",
    "            group_completions=batch_completions[q_i]\n",
    "            group_rewards=[]\n",
    "            group_old_policy_probs=[]\n",
    "            for o_i in range(G):\n",
    "                # reward\n",
    "                input_ids,input_text,completion_ids,completion_text=group_completions[o_i]\n",
    "                reward=reward_func(input_text,completion_text,batch_final_answers[q_i])\n",
    "                group_rewards.append(reward)\n",
    "                # old policy probs\n",
    "                old_policy_probs=gen_probs(policy,input_ids,completion_ids)\n",
    "                group_old_policy_probs.append(old_policy_probs.detach())\n",
    "            batch_old_policy_probs.append(group_old_policy_probs)\n",
    "            \n",
    "            # 组内优势计算\n",
    "            group_rewards=torch.tensor(group_rewards,dtype=policy.dtype)\n",
    "            group_advantages=(group_rewards-group_rewards.mean())/(group_rewards.std()+1e-8)\n",
    "            batch_group_advantages.append(group_advantages)\n",
    "        \n",
    "        # 训练N次\n",
    "        for times in range(1):\n",
    "            loss=0\n",
    "            for q_i in range(len(batch_queries)):\n",
    "                g_loss=0\n",
    "                group_completions=batch_completions[q_i]\n",
    "                group_old_policy_probs=batch_old_policy_probs[q_i]\n",
    "                group_advantages=batch_group_advantages[q_i]\n",
    "                group_ref_policy_probs=batch_ref_policy_probs[q_i]\n",
    "                for o_i in range(G):\n",
    "                    input_ids,input_text,completion_ids,completion_text=group_completions[o_i]\n",
    "                    # policy probs\n",
    "                    policy_probs=gen_probs(policy,input_ids,completion_ids)\n",
    "                    # kl\n",
    "                    KL=group_ref_policy_probs[o_i]/(policy_probs+1e-8)-torch.log(group_ref_policy_probs[o_i]/(policy_probs+1e-8))-1\n",
    "                    # compute loss\n",
    "                    o_loss=group_advantages[o_i]*(policy_probs/(group_old_policy_probs[o_i]+1e-8))-0.1*KL # B取0.1\n",
    "                    g_loss=g_loss+o_loss.sum()\n",
    "                loss=loss-g_loss/G\n",
    "            loss=loss/len(batch_queries)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss:',loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
